# Copyright 2018-2019 QuantumBlack Visual Analytics Limited## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND# NONINFRINGEMENT. IN NO EVENT WILL THE LICENSOR OR OTHER CONTRIBUTORS# BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF, OR IN# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.## The QuantumBlack Visual Analytics Limited ("QuantumBlack") name and logo# (either separately or in combination, "QuantumBlack Trademarks") are# trademarks of QuantumBlack. The License does not grant you any right or# license to the QuantumBlack Trademarks. You may not use the QuantumBlack# Trademarks or any confusingly similar mark as a trademark for your product,#     or use the QuantumBlack Trademarks in any other manner that might cause# confusion in the marketplace, including but not limited to in advertising,# on websites, or on software.## See the License for the specific language governing permissions and# limitations under the License.import pandas as pdimport requestsimport randomimport yamlfrom typing import Any, Dictfrom kedro.extras.datasets.pandas import CSVDataSetuser_agent_list = [    # Chrome    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',    # Firefox    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)']def get_proxy_credentials():    with open(r'conf/base/credentials.yml') as file:        credentials = yaml.full_load(file)        LUMINATI_PASS = credentials['luminati_cred']['LUMINATI_PASS']        LUMINATI_USER = credentials['luminati_cred']['LUMINATI_USER']        LUMINATI_HOST = credentials['luminati_cred']['LUMINATI_HOST']        LUMINATI_PORT = credentials['luminati_cred']['LUMINATI_PORT']    proxy = "{}:{}@{}:{}".format(LUMINATI_USER, LUMINATI_PASS, LUMINATI_HOST, LUMINATI_PORT)    return proxydef proxy_server():    proxies = {        "http": get_proxy_credentials(),        "https": get_proxy_credentials()    }    return proxiesdef hktvmall_conn_node(link: str) \        -> dict:    r = requests.get(link)    cook = []    for c in r.cookies:        cook.append("{}".format(c.name) + "=" + "{}".format(c.value))    user_agent = random.choice(user_agent_list)    headers = {        'Cookie': "; ".join(cook),        'User-Agent': user_agent,    }    return headersdef request_hktvmall_catagory_code(headers: dict, category_directory_url: str) \        -> list:    from lxml import html    category_directory_html = requests.get(category_directory_url).content    tree = html.fromstring(category_directory_html)    category_list = tree.xpath('//div[@class="directory-navbar"]/ul/a/li/@data-zone')    cat_list_raw = []    for i in category_list:        get_categories_url = "https://www.hktvmall.com/hktv/en/ajax/getCategories?categoryCode={}".format(i)        catalog_raw = requests.request("GET", get_categories_url, headers=headers).json()        for cat in catalog_raw['categories']:            cat['tagname'] = catalog_raw['tagname']            cat_list_raw.append(cat)    # list of json of Vertical    return cat_list_rawdef categories_df_etl(cat_list_raw: list) -> pd.DataFrame:    cat_list_etl = []    return pd.DataFrame(cat_list_etl)# main Category's top product (supermarket, beautynhealth etc)def gen_hktvmall_product_by_method_and_cat_links(categories: dict, methods: dict, url: str) \        -> Dict[str, Any]:    promotiondifference, hotpickorder = [], []    for code in categories.values():        method1_base_url = url.format(code, list(methods.values())[0], code, code) + "&pageSize=100&currentPage={}"        method2_base_url = url.format(code, list(methods.values())[1], code, code) + "&pageSize=100&currentPage={}"        for i in range(0, 100//60+1):            method1_real_url, method2_real_url = method1_base_url.format(str(i)), method2_base_url.format(str(i))            promotiondifference.append(                {'code': code, 'page_num': i, "url": method1_real_url}            )            hotpickorder.append(                {'code': code, 'page_num': i, "url": method2_real_url}            )    return dict(        method1={"type": list(methods.values())[0], "len": 100, "url_list": promotiondifference},        method2={"type": list(methods.values())[1], "len": 100, "url_list": hotpickorder}    )def gen_hktvmall_full_site_links(categories_df: pd.DataFrame, url: str) \        -> Dict[str, Any]:    # EDA: groupby('VerticalCode').sum().describe().unstack()    cat_count = categories_df.groupby('CategoryCode').sum()['Count']    all_url = []    for code, count in cat_count.iteritems():        base_url = url.format(code, code) + "&pageSize=60&currentPage={}"        for i in range(0, count // 60 + 1):            all_url.append({"url": base_url.format(str(i))})    return dict(url_list=all_url)def get_product_comment(headers: dict, product_code: list, comment_url: str, page_size_list: list) \        -> pd.DataFrame:    concatted_comment_raw = []    for code in product_code:        url = comment_url.format(code, random.choice(page_size_list))        comment_raw = requests.request("GET", url, headers=headers).json()['reviews']        concatted_comment_raw.append(pd.DataFrame(comment_raw))    return pd.concat(concatted_comment_raw, ignore_index=True)def single_request(url: str, headers: dict, proxies: dict) \        -> list:    return [x for x in requests.request("GET", url, headers=headers, proxies=proxies).json()['products']]def multi_threading_req(headers: dict, links_param: Dict) \        -> list:    proxies = proxy_server()    url_list = [item['url'] for item in links_param['url_list']]    import re    url_list = re.match()    random.shuffle(url_list)    url_list = url_list[:500]  # testing, DO NOT REMOVE THIS    import multiprocessing as mp    # only 80% of your cpu will be used    pool = mp.Pool(round(mp.cpu_count() * 0.8))    result_obj = [pool.apply_async(        func=single_request, args=(url, headers, proxies)    ) for url in url_list]    results = [r.get() for r in result_obj]    pool.close()    pool.join()    return [x for lt in results for x in lt]def raw_etl(raw: list) -> pd.DataFrame:    from collections import Counter    # find keys    key_dict = dict(Counter([key for product in raw for key in product]))    # unique_keys = [key for key, value in key_dict.items() if value != max(key_dict.values())]    common_keys = [key for key, value in key_dict.items() if value == max(key_dict.values())]    common_keys += ['description', 'averageRating', 'salesNumberString', 'savedPrice', 'other_keys']    dropping_keys = [        'categories', 'brandName', 'storeCodeOriginal', 'storeName', 'storeType', 'storeTypeDisplay', 'storeRating',        'deliveryModeName', 'storeTC', 'colors', 'promotionStyle', 'invisible', 'purchaseOption', 'quantity', 'userMax',        'store', 'packingLength', 'packingHeight', 'packingDepth',        'price', 'promotionPrice', 'recommendedPrice', 'numberOfColors',    ]    # formatting all keys    for product in raw:        other_keys = []        per_product_unique_keys = list(set(product.keys()) - set(common_keys) - set(dropping_keys))        # concatenating unique_keys into other_keys        if per_product_unique_keys:            for uni_key in per_product_unique_keys:                other_keys.append({uni_key: product[uni_key]})        product.update({"other_keys": other_keys})        # create all common_keys        for key in common_keys:            try:                product[key]            except KeyError:                product.update({key: None})                continue        # remove all dropping_keys and unique_keys        for key in list(set(dropping_keys) | set(per_product_unique_keys)):            try:                product.pop(key, None)            except KeyError:                continue    return pd.DataFrame(raw)def daily_supermarket_home_page(hktvmall_promo_url: str, page_type: str) -> list:    from random import shuffle    url_list = []    for i in range(1, 15):        url_list.append(hktvmall_promo_url.format(page_type, str(i)))    return shuffle(url_list)def df_to_kedro_csvdataset(df: pd.DataFrame, path: str) \        -> CSVDataSet:    data_set = CSVDataSet(filepath=path)    df = df[sorted(df.columns)]    data_set.save(df)    reloaded = data_set.load()    return reloadeddef make_scatter_plot(df: pd.DataFrame):    import matplotlib.pyplot as plt    fg, ax = plt.subplots()    for idx, item in enumerate(list(df.species.unique())):        df[df["species"] == item].plot.scatter(            x='petal_width', y='petal_length',            label=item, color=f"C{idx}", ax=ax        )    fg.set_size_inches(12, 12)    return fgdef split_data(data: pd.DataFrame, example_test_data_ratio: float) -> Dict[str, Any]:    """Node for splitting the classical Iris data set into training and test    sets, each split into features and labels.    The split ratio parameter is taken from conf/project/parameters.yml.    The data and the parameters will be loaded and provided to your function    automatically when the pipeline is executed and it is time to run this node.    """    data.columns = [        "sepal_length",        "sepal_width",        "petal_length",        "petal_width",        "target",    ]    classes = sorted(data["target"].unique())    # One-hot encoding for the target variable    data = pd.get_dummies(data, columns=["target"], prefix="", prefix_sep="")    # Shuffle all the data    data = data.sample(frac=1).reset_index(drop=True)    # Split to training and testing data    n = data.shape[0]    n_test = int(n * example_test_data_ratio)    training_data = data.iloc[n_test:, :].reset_index(drop=True)    test_data = data.iloc[:n_test, :].reset_index(drop=True)    # Split the data to features and labels    train_data_x = training_data.loc[:, "sepal_length":"petal_width"]    train_data_y = training_data[classes]    test_data_x = test_data.loc[:, "sepal_length":"petal_width"]    test_data_y = test_data[classes]    # When returning many variables, it is a good practice to give them names:    return dict(        train_x=train_data_x,        train_y=train_data_y,        test_x=test_data_x,        test_y=test_data_y,    )